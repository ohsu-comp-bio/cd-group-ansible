# Create CD-Group infrastructure instances and disks
# set OpenStack auth environment variables using RC file:
# http://docs.openstack.org/cli-reference/common/cli-set-environment-variables-using-openstack-rc.html
- name: launch a compute instances for CD Group infrastructure
  hosts: localhost
  gather_facts: false # don't gather facts on local host
  vars_files:
  - "vars/main.yml"
  vars:
    volume_name: "source control"
  tasks:
  - name: create portal
    os_server:
       state: present
       name: portal-main
       image: "Ubuntu 14.04" # choose an image that has python installed
       key_name: "{{ os_key }}"
       flavor: m1.xlarge
       network: "{{ os_network }}"
  - name: create DCC portal Elasticsearch cluster
    os_server:
       state: present
       name: "dcc-es-{{ item }}"
       image: "Ubuntu 14.04" # choose an image that has python installed
       key_name: "{{ os_key }}"
       flavor: acc.half
       network: "{{ os_network }}"
    with_items: ["1", "2", "3"]
  - name: create monitor
    os_server:
       state: present
       name: monitor
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: m1.xlarge
       network: "{{ os_network }}"
  - name: create source control instance
    os_server:
       state: present
       name: source-control
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: m1.small
       network: "{{ os_network }}"
       floating_ips: ["{{ source_control_ip }}"]
       volumes:
         - "{{ volume_name }}"
  - name: create single-node kafka cluster
    os_server:
       state: present
       name: kafka
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: m1.xlarge
       network: "{{ os_network }}"
       floating_ips: ["{{ kafka_ip }}"] # port open for kafka
  - name: create spark workers
    os_server:
       state: present
       name: "dcc-spark-worker-{{ item }}"
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: acc.quarter
       network: "{{ os_network }}"
    with_items: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  - name: add /etc/hosts entries for spark workers
    # needed for host IP resolution, because DNS is not installed in exastack
    lineinfile:
      dest: /etc/hosts
      line: "{{ hostvars[lookup('inventory_hostnames', 'dcc-spark-worker-' + item )].openstack.private_v4 }} {{ 'dcc-spark-worker-' + item }}"
    become: True
    with_items: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']
  - name: create HDFS name node
    os_server:
       state: present
       name: dcc-hdfs-namenode
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       # https://wiki.apache.org/hadoop/FAQ#What_kind_of_hardware_scales_best_for_Hadoop.3F
       # "dual processor/dual core machines with 4-8GB of RAM"
       # https://wiki.apache.org/hadoop/MachineScaling
       flavor: m1.large
       network: "{{ os_network }}"
  - name: create HDFS nodes
    os_server:
       state: present
       name: "dcc-hdfs-datanode-{{ item }}"
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       # https://wiki.apache.org/hadoop/FAQ#What_kind_of_hardware_scales_best_for_Hadoop.3F
       # "dual processor/dual core machines with 4-8GB of RAM"
       # https://wiki.apache.org/hadoop/MachineScaling
       flavor: m1.large
       network: "{{ os_network }}"
    with_items: ['1', '2', '3', '4']]
