# Create CD-Group infrastructure instances and disks
# set OpenStack auth environment variables using RC file: 
# http://docs.openstack.org/cli-reference/common/cli-set-environment-variables-using-openstack-rc.html
- name: launch a compute instances for CD Group infrastructure
  hosts: localhost
  gather_facts: false # don't gather facts on local host
  vars:
    volume_name: "source control"
  tasks:
  - name: create portal
    os_server:
       state: present
       name: portal-main
       image: "Ubuntu 14.04" # choose an image that has python installed
       key_name: "{{ os_key }}"
       flavor: m1.xlarge
       network: "{{ os_network }}"
  - name: create source control instance
    os_server:
       state: present
       name: source-control
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: m1.small
       network: "{{ os_network }}"
       floating_ips: ["{{ source_control_ip }}"]
       volumes:
       - "{{ volume_name }}" 
  - name: create single-node kafka cluster
    os_server:
       state: present
       name: kafka
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: m1.xlarge
       network: "{{ os_network }}"
       floating_ips: ["{{ kafka_ip }}"] # port open for kafka
  - name: create spark workers
    os_server:
       state: present
       name: "dcc-spark-worker-{{ item }}"
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: m1.xlarge
       network: "{{ os_network }}"
    with_items: [1, 2, 3, 4]
  - name: create HDFS nodes
    os_server:
       state: present
       name: "{{ item }}"
       image: "Ubuntu 14.04"
       key_name: "{{ os_key }}"
       flavor: m1.xlarge
       network: "{{ os_network }}"
    with_items:
      - dcc-hdfs-namenode
      - dcc-hdfs-datanode-1
      - dcc-hdfs-datanode-2
